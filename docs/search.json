[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Density Surface Modeling: A Case Study for Central Pacific False Killer Whales (Pseudorca crassidens)",
    "section": "",
    "text": "Devin Johnson, Erin Oleson, Janelle Badger, Amanda Bradford, Jennifer McCullough\nPacific Islands Fisheries Science Center, NOAA Fisheries, Honolulu, Hawaiʻi\n\nYvonne Barkley\nCooperative Institute for Marine & Atmospheric Research, University of Hawaiʻi at Mānoa, Honolulu, Hawaiʻi\n\nElizabeth Becker\nCascadia Research Collective LLC, Olympia, Washington\n\nMegan Wood\nSaltwater, Inc, Anchorage, Alaska\n\n\n\n\n\n\n\n\n\n\n\n\nSummary\nSpatially-explicit density surface models (DSMs) for cetaceans have become a powerful assessment and management tool enabling examination of animal distribution and density at various scales within the modeled area. The objective of the PIFSC Cetacean Research Program’s NPS Toolbox Initiative project is to update the central Pacific and Hawaiʻi DSM framework and produce a new density model for pelagic false killer whales, and other species, using data from the 2023 Hawaiian Islands Cetacean and Ecosystem Assessment Survey (HICEAS) survey. The project is using open science practices to accomplish three main objectives: (1) reformulation of the current central Pacific/Hawaiʻi DSM structure in R using upgraded spatial functionality and new modeled oceanographic products, (2) integration of new spatially-explicit methods for propagating uncertainty of detection parameters and environmental variability into stock assessments, and (3) exploration and integration of new model validation approaches, especially for data limited species. While the objectives of the CRP Toolbox project are to provide stock assessments for pelagic false killer whales in the central Pacific region, the overall goal is to provide a current, open access workflow for density surface modeling of distance sampling-based survey data that any region in NMFS can use for reproducible cetacean stock assessments. We will provide a discussion of the workflow, products, and current status of the project.\n\n\nR code\nR script files to execute the DSM analysis presented here can be found here in the the Github repository for the webpage.\n\n\nLicense\nAs a work by US federal employees as part of their official duties, this project is in the public domain within the United States of America. Additionally, we waive copyright and related rights in the work worldwide through the CC0 1.0 Universal public domain dedication.\n\n\nNOAA Disclaimer\nThis repository is a scientific product and is not official communication of the National Oceanic and Atmospheric Administration, or the United States Department of Commerce. All NOAA GitHub project code is provided on an ‘as is’ basis and the user assumes responsibility for its use. Any claims against the Department of Commerce or Department of Commerce bureaus stemming from the use of this GitHub project will be governed by all applicable Federal law. Any reference to specific commercial products, processes, or services by service mark, trademark, manufacturer, or otherwise, does not constitute or imply their endorsement, recommendation or favoring by the Department of Commerce. The Department of Commerce seal and logo, or the seal and logo of a DOC bureau, shall not be used in any manner to imply endorsement of any commercial product or activity by DOC or the United States Government.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "content/s6_extrapolation/index.html#section-1",
    "href": "content/s6_extrapolation/index.html#section-1",
    "title": "Assessing Areas of Extrapolation",
    "section": "Section",
    "text": "Section",
    "crumbs": [
      "6. Examine areas of extrapolation"
    ]
  },
  {
    "objectID": "content/s7_dsm_selection_fitting/index.html#section-1",
    "href": "content/s7_dsm_selection_fitting/index.html#section-1",
    "title": "Fitting and Selecting Density Surface Models",
    "section": "Section",
    "text": "Section",
    "crumbs": [
      "7. Fitting and selecting DSMs"
    ]
  },
  {
    "objectID": "content/s5_download_prediction_var/index.html",
    "href": "content/s5_download_prediction_var/index.html",
    "title": "Download Environmental Variables for Density Prediction",
    "section": "",
    "text": "library(here)\nlibrary(terra)\nlibrary(sf)\nlibrary(lubridate)\nlibrary(picMaps)\nlibrary(rnaturalearth)\nlibrary(reticulate) \nlibrary(tidyverse)\nlibrary(foreach)\nlibrary(foreach)\nlibrary(future)\nlibrary(doFuture)\nlibrary(progressr)\nlibrary(ggplot2)\nlibrary(ggspatial)\n\nlocalwd &lt;- here(\"_R_code\",\"task_5_download_prediction_data\")\ncopernicus_data &lt;- file.path(localwd, \"copernicus_data\")\npred_dir &lt;- file.path(localwd, \"output\")\nif(!dir.exists(copernicus_data)) dir.create(copernicus_data)\nif(!dir.exists(pred_dir)) dir.create(pred_dir)\n\nreticulate::use_virtualenv(\"cm_py\", required = TRUE)\ncm &lt;- reticulate::import(\"copernicusmarine\")\nlg &lt;- reticulate::import(\"logging\")\nlg$getLogger(\"copernicusmarine\")$setLevel('WARN')",
    "crumbs": [
      "5. Download environmental variables for prediction"
    ]
  },
  {
    "objectID": "content/s5_download_prediction_var/index.html#packages-and-python-preparation",
    "href": "content/s5_download_prediction_var/index.html#packages-and-python-preparation",
    "title": "Download Environmental Variables for Density Prediction",
    "section": "",
    "text": "library(here)\nlibrary(terra)\nlibrary(sf)\nlibrary(lubridate)\nlibrary(picMaps)\nlibrary(rnaturalearth)\nlibrary(reticulate) \nlibrary(tidyverse)\nlibrary(foreach)\nlibrary(foreach)\nlibrary(future)\nlibrary(doFuture)\nlibrary(progressr)\nlibrary(ggplot2)\nlibrary(ggspatial)\n\nlocalwd &lt;- here(\"_R_code\",\"task_5_download_prediction_data\")\ncopernicus_data &lt;- file.path(localwd, \"copernicus_data\")\npred_dir &lt;- file.path(localwd, \"output\")\nif(!dir.exists(copernicus_data)) dir.create(copernicus_data)\nif(!dir.exists(pred_dir)) dir.create(pred_dir)\n\nreticulate::use_virtualenv(\"cm_py\", required = TRUE)\ncm &lt;- reticulate::import(\"copernicusmarine\")\nlg &lt;- reticulate::import(\"logging\")\nlg$getLogger(\"copernicusmarine\")$setLevel('WARN')",
    "crumbs": [
      "5. Download environmental variables for prediction"
    ]
  },
  {
    "objectID": "content/s5_download_prediction_var/index.html#defining-the-central-pacific",
    "href": "content/s5_download_prediction_var/index.html#defining-the-central-pacific",
    "title": "Download Environmental Variables for Density Prediction",
    "section": "Defining the Central Pacific",
    "text": "Defining the Central Pacific\n\ncenpac &lt;- picMaps::cenpac()",
    "crumbs": [
      "5. Download environmental variables for prediction"
    ]
  },
  {
    "objectID": "content/s5_download_prediction_var/index.html#downloading-the-environmental-variables",
    "href": "content/s5_download_prediction_var/index.html#downloading-the-environmental-variables",
    "title": "Download Environmental Variables for Density Prediction",
    "section": "Downloading the environmental variables",
    "text": "Downloading the environmental variables\n\nyears &lt;- c(2017, 2020:2024)\nbound &lt;- sf::st_bbox(cenpac) \n\nwith_progress({\n  pb &lt;- progressor(along = years) \n  pred_download &lt;- foreach(i = seq_along(years), \n                           .options.future = list(seed = TRUE)) %do% \n    {\n      time_span &lt;- paste(years[i],c(\"-07-01\",\"-12-31\"), sep=\"\")\n      dataset_id &lt;- ifelse(years[i] &lt; 2021,\n                           \"cmems_mod_glo_phy_my_0.083deg_P1D-m\",\n                           \"cmems_mod_glo_phy_myint_0.083deg_P1D-m\")\n      \n      nc_dl &lt;- cm$subset(\n        dataset_id = dataset_id,\n        variables = as.list(c(\"mlotst\",\"so\",\"thetao\",\"zos\")),\n        minimum_longitude = bound$xmin - 1/6,\n        maximum_longitude = bound$xmax + 1/6,\n        minimum_latitude = bound$ymin - 1/6,\n        maximum_latitude = bound$ymax + 1/6,\n        start_datetime = time_span[1], \n        end_datetime = time_span[2], \n        minimum_depth = 0.5,\n        maximum_depth = 0.5,\n        coordinates_selection_method = \"outside\",\n        dataset_version = \"202311\",\n        disable_progress_bar = FALSE,\n        skip_existing = TRUE,\n        output_directory = copernicus_data\n      )\n      \n      pb()\n      data.frame(year=years[i], filename=nc_dl$filename)\n    }\n}) \n# plan(\"sequential\")\npred_download &lt;- do.call(rbind, pred_download)\nwrite.csv(pred_download, file.path(copernicus_data, \"pred_download.csv\"), row.names = FALSE)",
    "crumbs": [
      "5. Download environmental variables for prediction"
    ]
  },
  {
    "objectID": "content/s5_download_prediction_var/index.html#processing-the-raw-netcdf-files",
    "href": "content/s5_download_prediction_var/index.html#processing-the-raw-netcdf-files",
    "title": "Download Environmental Variables for Density Prediction",
    "section": "Processing the raw netCDF files",
    "text": "Processing the raw netCDF files\n\n# localwd &lt;- here(\"_R_code\",\"task_5_download_prediction_data\")\n# copernicus_data &lt;- file.path(localwd, \"copernicus_data\")\n# pred_dir &lt;- file.path(localwd, \"output\")\n# pred_download &lt;- read.csv(file.path(copernicus_data, \"pred_download.csv\"))\nyears &lt;- pred_download$year\n\nwith_progress({\n  pb &lt;- progressor(along = years) \n  for(i in seq_along(years)){\n    filenm &lt;- pred_download$filename[i]\n    env &lt;- terra::rast(file.path(copernicus_data, filenm))\n    vnms &lt;- terra::varnames(env) \n    new_vnms &lt;- c(\"mld\",\"salinity\",\"sst\",\"ssh\")\n    \n    dates &lt;- time(env) |&gt; unique() |&gt; sort()\n    subs_idx &lt;- which(rep(1:3, length=length(dates))==1)\n    \n    out_env &lt;- NULL\n    for(j in seq_along(vnms)){\n      tmp_env &lt;- toMemory(env[vnms[j]])\n      names(tmp_env) &lt;- rep(new_vnms[j],nlyr(tmp_env))\n      tmp_env &lt;- tmp_env[[subs_idx]]\n      tmp_env &lt;- terra::project(tmp_env, \"+proj=longlat +ellps=WGS84 +lon_wrap=180 +datum=WGS84 +no_defs\")\n      tmp_env &lt;- terra::focal(tmp_env, 3, mean, na.policy=\"only\", na.rm=TRUE) # Make sure there are no cells with missing data\n      out_env &lt;- c(out_env, tmp_env)\n    }\n    out_env &lt;- do.call(c, out_env)\n    # add focal sd\n    for(j in seq_along(vnms)){\n      tmp_env &lt;- out_env[[names(out_env)==new_vnms[j]]]\n      tmp_env_sd &lt;-  focal(tmp_env, 3, sd, na.rm=TRUE)\n      names(tmp_env_sd) &lt;- paste(names(tmp_env_sd),\"sd\",sep=\"_\")\n      out_env &lt;- c(out_env,tmp_env_sd)\n    }\n    writeRaster(out_env, file.path(pred_dir, paste0(\"cenpac_\",years[i], \"_env.tif\")), overwrite=TRUE)\n    pb()\n  }\n})\n\n\nPrediction data view for July, 31 2023",
    "crumbs": [
      "5. Download environmental variables for prediction"
    ]
  },
  {
    "objectID": "content/intro.html#density-surface-modeling",
    "href": "content/intro.html#density-surface-modeling",
    "title": "Introduction",
    "section": "Density Surface Modeling",
    "text": "Density Surface Modeling",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "content/s3_env_data/index.html",
    "href": "content/s3_env_data/index.html",
    "title": "Download and Merge Environmental data to segments",
    "section": "",
    "text": "A. Setup Python Environment\nCode to download data from the Copernicus Marine Data Store The user must create a login with a username and password to access their data. Username and password is required in the code below. This step is run only the first time when setting up the virtual Python env for data downloading.\n\nlibrary(reticulate) \n\n## Run this if Python is not installed on the machine\ninstall_python() \n\nreticulate::virtualenv_create(envname = \"cm_py\")\nreticulate::virtualenv_install(\"cm_py\", packages = c(\"copernicusmarine\"))\nreticulate::use_virtualenv(\"cm_py\", required = TRUE)\ncm &lt;- reticulate::import(\"copernicusmarine\")\n\n## Run once with your username and password #----\ncm$login(\"username\",\"password\")\n\n\n\nB. Download Copernicus Marine Data\n\nlibrary(here)\nlibrary(terra)\nlibrary(sf)\nlibrary(lubridate)\nlibrary(picMaps)\nlibrary(reticulate) \nlibrary(tidyverse)\nlibrary(foreach)\nlibrary(foreach)\nlibrary(future)\nlibrary(doFuture)\nlibrary(progressr)\n\nlocalwd &lt;- here(\"_R_code\",\"task_3_download_env_data\")\n\ncopernicus_data &lt;- file.path(localwd, \"copernicus_data\")\nif(!dir.exists(copernicus_data)) dir.create(copernicus_data)\n\nreticulate::use_virtualenv(\"cm_py\", required = TRUE)\ncm &lt;- reticulate::import(\"copernicusmarine\")\n\n\nsegout &lt;- readRDS(file.path(here(), \"_R_code\",\"task_2_est_g0_esw\",\"output\",\"seg_sight_out_g0_ESW.rds\"))\nsegdata &lt;- segout$segdata\nsegdata &lt;- st_as_sf(segdata, coords=c(\"mlon\",\"mlat\"), crs=4326) %&gt;% st_shift_longitude()\nsegdata &lt;- segdata %&gt;% mutate(\n  UTC = as.POSIXct(paste(year, month, day, sep = \"-\"), format = \"%Y-%m-%d\", tz = \"UTC\") + hms(mtime)\n) %&gt;% arrange(UTC)\n\n\nyears &lt;- unique(segdata$year)\n\nplan(\"multisession\", workers = 8) \n\nwith_progress({\n  pb &lt;- progressor(along = years) \n  env_download &lt;- foreach(i = seq_along(years), \n                          .errorhandling = \"pass\", \n                          .options.future = list(seed = TRUE)) %dofuture% \n    {\n      \n      ### Set up Python environment in each worker\n      cm &lt;- reticulate::import(\"copernicusmarine\")\n      lg &lt;- reticulate::import(\"logging\")\n      lg$getLogger(\"copernicusmarine\")$setLevel('WARN')\n      \n      tmp_seg &lt;- dplyr::filter(segdata, year==years[i]) \n      bound &lt;- st_bbox(tmp_seg) + c(-1/6, -1/6, 1/6, 1/6)\n      time_span &lt;- date(tmp_seg$UTC) %&gt;% range() %&gt;% as.character()\n      \n      dataset_id &lt;- ifelse(tmp_seg$year[1]&lt;=2021,\n                           \"cmems_mod_glo_phy_my_0.083deg_P1D-m\",\n                           \"cmems_mod_glo_phy_myint_0.083deg_P1D-m\")\n      \n      nc_dl &lt;- cm$subset(\n        dataset_id = dataset_id,\n        variables = as.list(c(\"mlotst\",\"so\",\"thetao\",\"zos\")),\n        minimum_longitude = bound$xmin,\n        maximum_longitude = bound$xmax,\n        minimum_latitude = bound$ymin,\n        maximum_latitude = bound$ymax,\n        start_datetime = time_span[1], \n        end_datetime = time_span[2], \n        minimum_depth = 0.5,\n        maximum_depth = 0.5,\n        coordinates_selection_method = \"outside\",\n        dataset_version = \"202311\",\n        disable_progress_bar = TRUE,\n        skip_existing = TRUE,\n        output_directory = copernicus_data\n      )\n      \n      pb()\n      \n      data.frame(years=years[i], filename=nc_dl$filename)\n    }\n  \n}) \nplan(\"sequential\")\nenv_download &lt;- do.call(rbind, env_download)\nwrite.csv(env_download, file.path(copernicus_data, \"env_download.csv\"), row.names = FALSE)\n\n\n\nC. Merge Compernicus Marine Data with Processed Segment Data\n\nyears &lt;- env_download$years\n\nsegdata_env &lt;- foreach(i = seq_along(years), \n                       .errorhandling = \"pass\", \n                       .options.future = list(seed = TRUE)) %do% \n  {\n    filenm &lt;- env_download$filename[i]\n    env &lt;- terra::rast(file.path(copernicus_data, filenm))\n    vnms &lt;- terra::varnames(env)\n    \n    tmp_seg &lt;- dplyr::filter(segdata, year==years[i]) \n    \n    for(j in seq_along(vnms)){\n      tmp_env &lt;- 1.0*env[vnms[j]]\n      tmp_env &lt;- terra::project(tmp_env, \"+proj=longlat +ellps=WGS84 +lon_wrap=180 +datum=WGS84 +no_defs\")\n      tmp_env &lt;- focal(tmp_env, 3, mean, na.policy=\"only\", na.rm=TRUE) # Make sure there are no cells with missing data\n      \n      ### Diagnostic plot if needed\n      # library(ggplot2); library(ggspatial)\n      # ggplot() +\n      #   layer_spatial(tmp_env[[1]]) +\n      #   layer_spatial(tmp_seg)\n      \n      tmp_ext &lt;- terra::extract(tmp_env, vect(tmp_seg))[,-1]\n      tmp_ext &lt;- tmp_ext[outer(date(tmp_seg$UTC), date(time(tmp_env)), \\(x,y) x-y==0)]\n      if(any(is.na(tmp_ext))) print(\"FYI: NAs in sd extraction!\") \n      tmp_seg &lt;- cbind(tmp_seg, tmp_ext)\n      colnames(tmp_seg)[colnames(tmp_seg)==\"tmp_ext\"] &lt;- vnms[j]\n      # Focal SD versions of habitat variables \n      tmp_env_sd &lt;- focal(tmp_env, 3, sd, na.rm=TRUE)\n      tmp_ext &lt;- terra::extract(tmp_env_sd, vect(tmp_seg))[,-1]\n      tmp_ext &lt;- tmp_ext[outer(date(tmp_seg$UTC), date(time(tmp_env)), \\(x,y) x-y==0)]\n      if(any(is.na(tmp_ext))) print(\"FYI: NAs in sd extraction!\") \n      tmp_seg &lt;- cbind(tmp_seg, tmp_ext)\n      colnames(tmp_seg)[colnames(tmp_seg)==\"tmp_ext\"] &lt;- paste0(vnms[j],\"_sd\")\n    }\n    \n    tmp_seg\n  }\n\nsegdata_env &lt;- do.call(rbind, segdata_env)\nsegdata_env$Longitude &lt;- st_coordinates(segdata_env)[,1]\nsegdata_env$Latitude &lt;- st_coordinates(segdata_env)[,2]\nsegdata_env &lt;- st_drop_geometry(segdata_env)\n\n## Arrange segments by time (UTC)\nsegdata_env &lt;- arrange(segdata_env, UTC)\n\n## Reorder and rename covariates\ncols_to_modify &lt;- c(\"thetao\", \"thetao_sd\", \"so\", \"so_sd\", \"mlotst\", \"mlotst_sd\", \"zos\", \"zos_sd\")\n\nsegdata_env &lt;- segdata_env %&gt;% select(-all_of(cols_to_modify), all_of(cols_to_modify) ) %&gt;% \n  rename(sst = thetao, sst_sd=thetao_sd, salinity=so, salinity_sd=so_sd, mld=mlotst, mld_sd=mlotst_sd, ssh=zos, ssh_sd=zos_sd)\n\nwrite.csv(segdata_env, file = file.path(localwd,\"output\",\"segdata_env.csv\"), row.names = FALSE)\nsaveRDS(segdata_env, file= file.path(localwd,\"output\",\"segdata_env.rds\") )",
    "crumbs": [
      "3. Dowload envrion. data"
    ]
  },
  {
    "objectID": "content/s4_fit_models/index.html",
    "href": "content/s4_fit_models/index.html",
    "title": "Fit and Select Density Surface Models (DSM) using GAMs",
    "section": "",
    "text": "localwd &lt;- here(\"_R_code\",\"task_4_fit_dsm_models\")\n\nsegDataESWg0 &lt;- readRDS( here(\"_R_code\",\"task_2_est_g0_esw\",\"output\",\"seg_sight_out_g0_ESW.rds\") )\n\n#' Read table defining which sightings are from the pelagic population\nfkw_popData &lt;- readRDS( here(\"_R_code\", \"task_4_fit_dsm_models\", \"FKWsights_DSM_1997to2023.rds\") ) \n\nspcode &lt;- '033' # FKW code\nsegDataESWg0$sightinfo &lt;- segDataESWg0$sightinfo %&gt;% filter(SpCode==spcode)%&gt;%\n  dplyr::mutate(CruiseSightNo = paste(Cruise,SightNo,sep=\"-\"))%&gt;%\n  filter(CruiseSightNo %in% fkw_popData$CruiseSightNo)\n\n#' Add probability sighting is pelagic stock \nsegDataESWg0$sightinfo &lt;- segDataESWg0$sightinfo %&gt;% left_join(.,fkw_popData[,c(\"CruiseSightNo\",\"ProbPel\",\"EffGrpSizeFull\",\"EffGrpSizePelagic\")], by=\"CruiseSightNo\") \nsegDataESWg0$sightinfo$nSI_033 &lt;- 1\nsegDataESWg0$sightinfo$ANI_033 &lt;- segDataESWg0$sightinfo$EffGrpSizeFull\n\nsightinfo &lt;- segDataESWg0$sightinfo %&gt;% select(segnum, ProbPel, nSI_033, ANI_033)\n\nsegDataESWg0$segdata &lt;- select(segDataESWg0$segdata, -nSI_033, -ANI_033)\nsegDataESWg0$segdata &lt;- left_join(segDataESWg0$segdata, sightinfo)\nsegDataESWg0$segdata &lt;- segDataESWg0$segdata %&gt;% mutate(\n  nSI_033 = ifelse(is.na(nSI_033), 0, nSI_033),\n  ANI_033 = ifelse(is.na(ANI_033), 0, ANI_033),\n  ProbPel = ifelse(is.na(ProbPel), 1, ProbPel)\n)\n\nwrite.csv(segDataESWg0$sightinfo, file.path(localwd, \"output\", \"sightinfo_033.csv\" ), row.names = FALSE)\n\n\n\n\n\nsegEnvData &lt;- readRDS(file.path(here(), \"_R_code\",\"task_3_download_env_data\",\"output\",\"segdata_env.rds\")) %&gt;% \n  select(segnum, Latitude, Longitude, UTC, sst:ssh_sd)\nsegdata &lt;- left_join(segDataESWg0$segdata, segEnvData, by=\"segnum\")\n\nsaveRDS(segdata, file.path(localwd, \"output\", \"fkw_dsm_data_1997to2024.rds\") )\nwrite.csv(segdata, file.path(localwd, \"output\", \"fkw_dsm_data_1997to2024.csv\"), row.names = FALSE)",
    "crumbs": [
      "4. Fit DSM models"
    ]
  },
  {
    "objectID": "content/s4_fit_models/index.html#data-preparation",
    "href": "content/s4_fit_models/index.html#data-preparation",
    "title": "Fit and Select Density Surface Models (DSM) using GAMs",
    "section": "",
    "text": "localwd &lt;- here(\"_R_code\",\"task_4_fit_dsm_models\")\n\nsegDataESWg0 &lt;- readRDS( here(\"_R_code\",\"task_2_est_g0_esw\",\"output\",\"seg_sight_out_g0_ESW.rds\") )\n\n#' Read table defining which sightings are from the pelagic population\nfkw_popData &lt;- readRDS( here(\"_R_code\", \"task_4_fit_dsm_models\", \"FKWsights_DSM_1997to2023.rds\") ) \n\nspcode &lt;- '033' # FKW code\nsegDataESWg0$sightinfo &lt;- segDataESWg0$sightinfo %&gt;% filter(SpCode==spcode)%&gt;%\n  dplyr::mutate(CruiseSightNo = paste(Cruise,SightNo,sep=\"-\"))%&gt;%\n  filter(CruiseSightNo %in% fkw_popData$CruiseSightNo)\n\n#' Add probability sighting is pelagic stock \nsegDataESWg0$sightinfo &lt;- segDataESWg0$sightinfo %&gt;% left_join(.,fkw_popData[,c(\"CruiseSightNo\",\"ProbPel\",\"EffGrpSizeFull\",\"EffGrpSizePelagic\")], by=\"CruiseSightNo\") \nsegDataESWg0$sightinfo$nSI_033 &lt;- 1\nsegDataESWg0$sightinfo$ANI_033 &lt;- segDataESWg0$sightinfo$EffGrpSizeFull\n\nsightinfo &lt;- segDataESWg0$sightinfo %&gt;% select(segnum, ProbPel, nSI_033, ANI_033)\n\nsegDataESWg0$segdata &lt;- select(segDataESWg0$segdata, -nSI_033, -ANI_033)\nsegDataESWg0$segdata &lt;- left_join(segDataESWg0$segdata, sightinfo)\nsegDataESWg0$segdata &lt;- segDataESWg0$segdata %&gt;% mutate(\n  nSI_033 = ifelse(is.na(nSI_033), 0, nSI_033),\n  ANI_033 = ifelse(is.na(ANI_033), 0, ANI_033),\n  ProbPel = ifelse(is.na(ProbPel), 1, ProbPel)\n)\n\nwrite.csv(segDataESWg0$sightinfo, file.path(localwd, \"output\", \"sightinfo_033.csv\" ), row.names = FALSE)\n\n\n\n\n\nsegEnvData &lt;- readRDS(file.path(here(), \"_R_code\",\"task_3_download_env_data\",\"output\",\"segdata_env.rds\")) %&gt;% \n  select(segnum, Latitude, Longitude, UTC, sst:ssh_sd)\nsegdata &lt;- left_join(segDataESWg0$segdata, segEnvData, by=\"segnum\")\n\nsaveRDS(segdata, file.path(localwd, \"output\", \"fkw_dsm_data_1997to2024.rds\") )\nwrite.csv(segdata, file.path(localwd, \"output\", \"fkw_dsm_data_1997to2024.csv\"), row.names = FALSE)",
    "crumbs": [
      "4. Fit DSM models"
    ]
  },
  {
    "objectID": "content/s4_fit_models/index.html#dsm-formulation-and-fitting",
    "href": "content/s4_fit_models/index.html#dsm-formulation-and-fitting",
    "title": "Fit and Select Density Surface Models (DSM) using GAMs",
    "section": "DSM Formulation And Fitting",
    "text": "DSM Formulation And Fitting\n\nLoad and add effort calculation to data\n\nlocal_wd &lt;- file.path(here(), \"_R_code\", \"task_4_fit_dsm_models\")\nfkw_dsm_data &lt;- readRDS(file.path(local_wd, \"output\", \"fkw_dsm_data_1997to2024.rds\"))\n\n#' Add effort data\nfkw_dsm_data &lt;- fkw_dsm_data %&gt;% mutate(\n  effort = g0_033 * 2 * esw_033 * dist\n)\n\n#' Filter out years without any sightings\nnz_sight &lt;- fkw_dsm_data %&gt;% group_by(year) %&gt;% summarize(sightings = sum(nSI_033)) %&gt;% \n  filter(sightings&gt;0)\n\nfkw_dsm_data &lt;- fkw_dsm_data %&gt;% filter(year%in%nz_sight$year, beaufort&lt;7, effort&gt;0)\nfkw_gs &lt;- fkw_dsm_data %&gt;% filter(nSI_033 &gt; 0)# Just segments with sightings\n\n\n\nAdd augmented data for pelagic probabilty weighting\n\nnonpel &lt;- fkw_dsm_data %&gt;% filter(ProbPel&lt;1)\nnonpel &lt;- nonpel %&gt;% mutate(\n  nSI_033 = 0,\n  ProbPel = 1-ProbPel,\n  augment=1\n)\n\nfkw_dsm_data &lt;- bind_rows(fkw_dsm_data, nonpel) %&gt;% \n  mutate(\n    augment = ifelse(is.na(augment), 0, 1)\n  )\n\n\n\nCreate model list and fit\n\nEncounter rate models\nforms_er and forms_gs\n\nhab_var &lt;- c(\"sst\",\"ssh\",\"mld\",\"salinity\")\nhab_var_sd &lt;- c(\"sst_sd\",\"ssh_sd\",\"mld_sd\",\"salinity_sd\")\n\nhab_inc &lt;- expand.grid(rep(list(0:2), length(hab_var)))\nhab_inc &lt;- hab_inc[apply(hab_inc==2, 1, sum)&lt;2,]\nhab_sd_inc &lt;- expand.grid(rep(list(0:1), length(hab_var_sd)))\n\nform_hab &lt;- apply(hab_inc, 1, function(row) {\n  terms &lt;- mapply(function(choice, var) {\n    if (choice == 1) return(paste0(\"s(\",var,\",bs=spline2use,k=5)\"))\n    if (choice == 2) return(paste0(\"te(\", var, \",Latitude,bs=spline2use,k=5)\"))\n    return(NULL)\n  }, row, hab_var)\n  terms &lt;- unlist(terms)\n  if (length(terms) == 0) return(\"nSI_033 ~ \")  # skip null model\n  paste(\"nSI_033 ~\", paste(terms, collapse = \" + \"))\n}) %&gt;% unlist()\n\nform_hab_sd &lt;- apply(hab_sd_inc, 1, function(row) {\n  terms &lt;- mapply(function(choice, var) {\n    if (choice == 1) return(paste0(\"s(\",var,\",bs=spline2use,k=5)\"))\n    return(NULL)\n  }, row, hab_var_sd)\n  terms &lt;- unlist(terms)\n  # if (length(terms) == 0) return(NULL)  # skip null model\n  paste(terms, collapse = \" + \")\n}) %&gt;% unlist()\n\nforms_er &lt;- NULL\nfor(i in seq_along(form_hab)){\n  if(i ==1) {\n    tmp &lt;- paste(form_hab[i], form_hab_sd[-1], sep=\"\")\n  } else {\n    tmp &lt;- c(form_hab[i], paste(form_hab[i], form_hab_sd[-1], sep=\" + \"))\n  }\n  forms_er &lt;- c(forms_er, tmp)\n}\n\n\n\nFit all ER models\n\n#' Select spline type\n#' 'ts'= thin plate splines (\"shrinkage approach\" applies additional smoothing penalty)\n#' \"tp\" is default for s()\nspline2use &lt;- \"ts\" \n\nworkers &lt;- 8\nplan(\"multisession\", workers=workers)\n# forms &lt;- split(forms, cut(seq_along(forms), breaks = workers, labels = FALSE))\n\nwith_progress({\n  pb &lt;- progressor(along = forms_er) \n  ergam_list &lt;- foreach(\n    i = seq_along(forms_er), .options.future = list(seed = TRUE), \n    .errorhandling = \"pass\") %dofuture% {\n      spline2use &lt;- spline2use\n      gam_fit &lt;- gam(formula = as.formula(forms_er[i]), offset = log(effort),\n                     family = tw(),\n                     method=\"REML\", \n                     data = fkw_dsm_data, weights = ProbPel)\n      pb()\n      list(summary = summary(gam_fit), aic=gam_fit$aic)\n    }\n})\nplan(\"sequential\")\n\nmod_df &lt;- data.frame(\n  idx = 1:length(ergam_list),\n  form = forms_er, \n  aic = sapply(ergam_list, \\(x) x$aic), \n  dev_expl=sapply(ergam_list, \\(x) x$summary$dev.expl*100)\n) %&gt;% mutate(\n  waic = exp(-0.5*(aic-min(aic))),\n  waic = waic/sum(waic)\n) %&gt;% arrange(desc(waic))\n\n\n\nGroup size model\n\nhab_inc_gs &lt;- expand.grid(rep(list(0:1), length(hab_var)))\nforms_gs &lt;- apply(hab_inc_gs, 1, function(row) {\n  terms &lt;- mapply(function(choice, var) {\n    if (choice == 1) return(paste0(\"s(\",var,\",bs=spline2use,k=5)\"))\n    return(NULL)\n  }, row, hab_var)\n  terms &lt;- unlist(terms)\n  # if (length(terms) == 0) return(NULL)  # skip null model\n  paste(terms, collapse = \" + \")\n}) %&gt;% unlist()\nforms_gs &lt;- paste(\"log(ANI_033)\", forms_gs, sep=\" ~ \")\nforms_gs[1] &lt;- \"log(ANI_033) ~ 1\"\n\n\n\nFit all GS models\n\ngsgam_list &lt;- foreach(\n  i = seq_along(forms_gs), .options.future = list(seed = TRUE), \n  .errorhandling = \"pass\") %do% {\n    spline2use &lt;- spline2use\n    gam_fit &lt;- gam(formula = as.formula(forms_gs[i]),\n                   method=\"REML\", \n                   data = fkw_gs, weights = ProbPel)\n    list(summary = summary(gam_fit), aic=gam_fit$aic)\n  }\n\nmod_df_gs &lt;- data.frame(\n  idx = 1:length(gsgam_list),\n  form = forms_gs, \n  aic = sapply(gsgam_list, \\(x) x$aic), \n  dev_expl=sapply(gsgam_list, \\(x) x$summary$dev.expl*100)\n) %&gt;% mutate(\n  waic = exp(-0.5*(aic-min(aic))),\n  waic = waic/sum(waic)\n) %&gt;% arrange(desc(waic))\n\n\n\n\nAIC tables\n\n\n# A tibble: 767 × 6\n    ...1   idx form                                         aic dev_expl    waic\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                                      &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1     1    90 nSI_033 ~ te(sst,Latitude,bs=spline2use,k…  398.     5.93 0.00638\n 2     2    91 nSI_033 ~ te(sst,Latitude,bs=spline2use,k…  398.     5.93 0.00638\n 3     3   411 nSI_033 ~ te(sst,Latitude,bs=spline2use,k…  398.     5.93 0.00638\n 4     4    94 nSI_033 ~ te(sst,Latitude,bs=spline2use,k…  398.     5.93 0.00638\n 5     5   218 nSI_033 ~ te(sst,Latitude,bs=spline2use,k…  398.     5.93 0.00638\n 6     6   219 nSI_033 ~ te(sst,Latitude,bs=spline2use,k…  398.     5.93 0.00638\n 7     7   414 nSI_033 ~ te(sst,Latitude,bs=spline2use,k…  398.     5.93 0.00638\n 8     8   410 nSI_033 ~ te(sst,Latitude,bs=spline2use,k…  398.     5.93 0.00638\n 9     9    95 nSI_033 ~ te(sst,Latitude,bs=spline2use,k…  398.     5.93 0.00638\n10    10   538 nSI_033 ~ te(sst,Latitude,bs=spline2use,k…  398.     5.93 0.00638\n# ℹ 757 more rows\n\n\n# A tibble: 16 × 5\n     idx form                                              aic  dev_expl    waic\n   &lt;dbl&gt; &lt;chr&gt;                                           &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1     7 log(ANI_033) ~ s(ssh,bs=spline2use,k=5) + s(ml…  85.7  2.88e+ 1 2.38e-1\n 2     8 log(ANI_033) ~ s(sst,bs=spline2use,k=5) + s(ss…  85.7  2.88e+ 1 2.38e-1\n 3    16 log(ANI_033) ~ s(sst,bs=spline2use,k=5) + s(ss…  85.7  2.88e+ 1 2.38e-1\n 4    15 log(ANI_033) ~ s(ssh,bs=spline2use,k=5) + s(ml…  85.7  2.88e+ 1 2.38e-1\n 5    13 log(ANI_033) ~ s(mld,bs=spline2use,k=5) + s(sa…  92.8  1.24e+ 1 6.81e-3\n 6    14 log(ANI_033) ~ s(sst,bs=spline2use,k=5) + s(ml…  92.8  1.24e+ 1 6.81e-3\n 7     5 log(ANI_033) ~ s(mld,bs=spline2use,k=5)          92.8  1.23e+ 1 6.81e-3\n 8     6 log(ANI_033) ~ s(sst,bs=spline2use,k=5) + s(ml…  92.8  1.23e+ 1 6.81e-3\n 9     3 log(ANI_033) ~ s(ssh,bs=spline2use,k=5)          93.6  1.07e+ 1 4.63e-3\n10    11 log(ANI_033) ~ s(ssh,bs=spline2use,k=5) + s(sa…  93.6  1.07e+ 1 4.63e-3\n11     4 log(ANI_033) ~ s(sst,bs=spline2use,k=5) + s(ss…  93.6  1.07e+ 1 4.63e-3\n12    12 log(ANI_033) ~ s(sst,bs=spline2use,k=5) + s(ss…  93.6  1.07e+ 1 4.63e-3\n13     1 log(ANI_033) ~ 1                                 96.9 -3.62e-14 8.85e-4\n14     9 log(ANI_033) ~ s(salinity,bs=spline2use,k=5)     96.9  1.73e- 3 8.85e-4\n15    10 log(ANI_033) ~ s(sst,bs=spline2use,k=5) + s(sa…  96.9  4.62e- 4 8.85e-4\n16     2 log(ANI_033) ~ s(sst,bs=spline2use,k=5)          96.9  1.45e- 4 8.85e-4\n\n\n\nwrite.csv(mod_df, file = file.path(local_wd, \"output\", \"aic_er.csv\"))\nwrite.csv(mod_df_gs, file = file.path(local_wd, \"output\" \"aic_gs.csv\"))\nsave( fkw_dsm_data, fkw_gs, spline2use, file=file.path(local_wd, \"output\",\"dsm_model_data.RData\"))",
    "crumbs": [
      "4. Fit DSM models"
    ]
  },
  {
    "objectID": "content/s1_construct_segs/index.html",
    "href": "content/s1_construct_segs/index.html",
    "title": "Segmenting DAS Survey Data",
    "section": "",
    "text": "The first step in our workflow is to process raw DAS data collected during ship-based line-transect surveys, and break effort into discrete segments for variance estimation. Here we use swfscdas package tools to load, process, and segment these DAS files, but this functionality exists in other packages such as LTabundR.",
    "crumbs": [
      "1. Segmenting DAS Survey Data"
    ]
  },
  {
    "objectID": "content/s1_construct_segs/index.html#load-packages-and-define-working-directory",
    "href": "content/s1_construct_segs/index.html#load-packages-and-define-working-directory",
    "title": "Segmenting DAS Survey Data",
    "section": "Load packages and define working directory",
    "text": "Load packages and define working directory\n\nlibrary(here)\nlibrary(swfscDAS)\nlibrary(tidyverse)\n\nlocalwd &lt;- here(\"_R_code\",\"task_1_segment_das_data\")\ndas_file &lt;- file.path(localwd, \"CenPac_Toolbox_1997-2023.das\")\nstrata_file &lt;- file.path(localwd, \"CenPac2.csv\")\n# randPics &lt;- readRDS(file.path(localwd, \"output\",\"seg_sight_out.rds\"))[[\"randpicks\"]] # Run this to maintain segments from previous processing of same das file, and load using argument below\nsp.code &lt;- \"033\" # false killer whale species code",
    "crumbs": [
      "1. Segmenting DAS Survey Data"
    ]
  },
  {
    "objectID": "content/s1_construct_segs/index.html#process-raw-das-data-into-segments-for-dsm-analysis",
    "href": "content/s1_construct_segs/index.html#process-raw-das-data-into-segments-for-dsm-analysis",
    "title": "Segmenting DAS Survey Data",
    "section": "Process raw DAS data into segments for DSM analysis",
    "text": "Process raw DAS data into segments for DSM analysis\n\ny.proc &lt;- swfscDAS::das_process(das_file)\n\ny.eff &lt;- swfscDAS::das_effort(\n  y.proc, \n  method = \"equallength\", \n  seg.km = 10,\n  dist.method = \"greatcircle\",\n  num.cores = 1, \n  strata_files = strata_file\n  # , randpicks.load = randPics \n)\n\ny.eff.sight &lt;- swfscDAS::das_effort_sight(y.eff, sp.codes = sp.code, sp.events = \"S\") # Arguments define which sightings to summarize \n\ny.eff.sight.strata &lt;- swfscDAS::das_intersects_strata(y.eff.sight, list(InPoly = strata_file)) # Provides logical column to filter for strata \n\n#Filter for study area\ny.eff.sight.strata$segdata &lt;- y.eff.sight.strata$segdata %&gt;% filter(InPoly == 1)\ny.eff.sight.strata$sightinfo &lt;- y.eff.sight.strata$sightinfo %&gt;% filter(InPoly == 1)\n\n#filter sighting info for S,G events\ny.eff.sight.strata$sightinfo &lt;- y.eff.sight.strata$sightinfo%&gt;%filter(Event %in% c(\"S\",\"G\"))\n\n\nData editing, filtering for errors\n\n#Filter for segment lengths &gt; 0 that do not have an associated sighting (threshold distance  = 0.1)\nzero_dist_segs &lt;- y.eff.sight.strata$segdata$segnum[y.eff.sight.strata$segdata$dist&lt;0.1] #define which segments are less than threshold\n\n\nfor(k in 1:length(zero_dist_segs)){ # For those segments less than threshold, \n  segnum_k &lt;- zero_dist_segs[k]\n  y.eff.sight.strata$segdata$dist[y.eff.sight.strata$segdata$segnum==segnum_k] &lt;-  ifelse(y.eff.sight.strata$segdata$nSI_033[y.eff.sight.strata$segdata$segnum==segnum_k] &gt; 0, 0.11,0) # If there is a sighting, increase distance to 0.11 to avoid filtering out\n}\n\ny.eff.sight.strata$segdata &lt;- y.eff.sight.strata$segdata %&gt;% filter(dist &gt; 0.1)\n\n# Error in data entry resulted in NA values in avgBft -- fix with Bft values provided from das file \nif(any(is.na(y.eff.sight.strata$segdata$avgBft))){\n  fix_bft&lt;-data.frame(NA_bft = which(is.na(y.eff.sight.strata$segdata$avgBft)), fixed = c(5,6,6,5,5,4,4)) \n  y.eff.sight.strata$segdata[fix_bft$NA_bft,\"avgBft\"] &lt;- fix_bft$fixed\n}",
    "crumbs": [
      "1. Segmenting DAS Survey Data"
    ]
  },
  {
    "objectID": "content/s1_construct_segs/index.html#save-processed-segment-data",
    "href": "content/s1_construct_segs/index.html#save-processed-segment-data",
    "title": "Segmenting DAS Survey Data",
    "section": "Save processed segment data",
    "text": "Save processed segment data\n\nif (!dir.exists(file.path(localwd,\"output\"))) {\n  dir.create(file.path(localwd,\"output\"))\n}\n\nsaveRDS(y.eff.sight.strata, file.path(localwd,\"output\", \"seg_sight_out.rds\"))",
    "crumbs": [
      "1. Segmenting DAS Survey Data"
    ]
  },
  {
    "objectID": "content/s2_est_g0_esw/index.html",
    "href": "content/s2_est_g0_esw/index.html",
    "title": "Estimate g(0) and Effective Strip Width with Auxillary Data",
    "section": "",
    "text": "A. Process Previous Survey DAS Data\n\nlibrary(tidyverse)\nlibrary(Distance)\nlibrary(lubridate)\nlibrary(here)\nlibrary(swfscDAS)\nlibrary(MASS)\nlibrary(mvnfast)\nlibrary(sf)\nlibrary(picMaps)\n\nlocalwd &lt;- here(\"_R_code\",\"task_2_est_g0_esw\")\nsp.code &lt;- \"033\"\n\n\nRead in previous survey data\n\n#read in input files\ndas_file &lt;- file.path(localwd, \"AllSurveys_g0_ESW_1986-2023.das\")\ny.proc &lt;- swfscDAS::das_process(das_file)\nstrata_file_dir &lt;- file.path(localwd, \"Strata Files\", \"region_filter_esw_g0.csv\")\n\n\n\nProcess DAS file and filter for FKW regions\n\ny.eff &lt;- swfscDAS::das_effort(\n  y.proc, method = \"equallength\",\n  seg.km = 10,\n  dist.method = \"greatcircle\",\n  num.cores = 1,\n  strata_files = strata_file_dir)\ny.eff.sight &lt;- swfscDAS::das_effort_sight(y.eff, sp.codes = sp.code, sp.events = \"S\")\ny.eff.sight.strata &lt;- swfscDAS::das_intersects_strata(y.eff.sight, list(InPoly = strata_file_dir)) \n\n\n\nExport data for use in g(0) and ESW estimation\n\ninput_dat&lt;-list()\ninput_dat$esw.dat &lt;- y.eff.sight.strata$sightinfo %&gt;% \n  filter(SpCode == sp.code, InPoly == 1, OnEffort == TRUE)\ninput_dat$g0.dat &lt;- y.eff.sight.strata$segdata %&gt;% filter(InPoly == 1, EffType == \"S\")\n\nsaveRDS(input_dat, file.path(localwd, \"output\",\"esw_g0_input.rds\"))\n\n\n\n\nB. Estimate ESW and g(0) Relative to Beaufort Sea State\nIf you are not continuing from step A in the same session, the following code must be executed to load the appropriate packages and processed data from setp A. Otherwise, this code chunk can be skipped.\n\n#' load packages\nlibrary(tidyverse)\nlibrary(Distance)\nlibrary(lubridate)\nlibrary(here)\nlibrary(mvnfast)\nlibrary(picMaps)\nlibrary(future)\nlibrary(doFuture)\n\nlocalwd &lt;- here(\"_R_code\",\"task_2_est_g0_esw\")\nsp.code &lt;- \"033\"\n\n\n#' Recall saved data from step 1\ninput_dat &lt;- readRDS(file.path(localwd, \"output\",\"esw_g0_input.rds\"))\n\n\nEstimate Effective Strip Width (ESW) relative to Beaufort sea state\n\ndf &lt;- data.frame(distance = input_dat$esw.dat$PerpDistKm, beaufort = input_dat$esw.dat$Bft)\ndf &lt;- df[complete.cases(df),]\n\ndetfun &lt;- ds(df, formula = ~beaufort, truncation = 5.5, key=\"hn\") \n\nModel contains covariate term(s): no adjustment terms will be included.\n\n\nFitting half-normal key function\n\n\nAIC= 382.087\n\n\nNo survey area information supplied, only estimating detection function.\n\nsummary(detfun)\n\n\nSummary for distance analysis \nNumber of observations :  132 \nDistance range         :  0  -  5.5 \n\nModel       : Half-normal key function \nAIC         :  382.087 \nOptimisation:  mrds (nlminb) \n\nDetection function parameters\nScale coefficient(s):  \n              estimate         se\n(Intercept)  1.2486795 0.22333973\nbeaufort    -0.1320774 0.05168352\n\n                       Estimate          SE         CV\nAverage p             0.4805914  0.02942133 0.06121901\nN in covered region 274.6616170 24.21359682 0.08815792\n\nplot(detfun, showpoints = TRUE)\n\n\n\n\n\n\n\n\nEstimate ESW for all segments\n\nmodeling_data &lt;- readRDS(file.path(here(), \"_R_code\", \"task_1_segment_das_data\", \"output\",\"seg_sight_out.rds\"))\nmodeling_data$segdata &lt;- modeling_data$segdata %&gt;% rename(beaufort = avgBft)\nmodeling_data$segdata[paste0(\"esw\",\"_\",sp.code)] &lt;- predict(detfun, modeling_data$segdata, esw=TRUE)[[\"fitted\"]]\n\n\n\nEstimate g(0) relative to Beaufort sea state\n\ng0_dat &lt;- input_dat$g0.dat\ng0_dat$resp &lt;- as.logical(g0_dat[,paste0(\"nSI_\",sp.code)])\ng0_dat &lt;- g0_dat%&gt;%dplyr::select(resp, mlat, mlon, avgBft, year,dist) %&gt;% \n  rename(beaufort = avgBft)\ng0_dat &lt;- g0_dat[complete.cases(g0_dat), ]\n\ng0_dat$esa &lt;- predict(detfun, newdata = g0_dat, esw=TRUE)[[\"fitted\"]] * 2 * g0_dat$dist\ng0_dat &lt;- filter(g0_dat, esa&gt;0)\n\n#' Combine beaufort=0,1 as the same g(0) = 1\ng0_est&lt;-scam::scam(formula = resp ~ s(I(pmax(beaufort-1,0)), bs='mpd', k=4)\n                     + s(mlat, mlon, bs='tp') + s(year, k=4), \n                   offset = log(esa+0.0001),\n                   family=poisson,\n                   data=g0_dat,\n                   not.exp=TRUE,\n                   gamma=1.4)\n\nEstimate relative g(0) for each Beaufort sea state\n\nnewdata_g0 &lt;- data.frame(beaufort = 0:7,\n                         mlat = mean(modeling_data$segdata$mlat, na.rm=TRUE),\n                         mlon = mean(modeling_data$segdata$mlon, na.rm=TRUE),\n                         year = round(mean(modeling_data$segdata$year, na.rm=TRUE)))\n\nps &lt;- scam::predict.scam(g0_est, newdata = newdata_g0, type=\"response\")\nRg0 &lt;- ps/ps[1]\nnames(Rg0) &lt;- 0:7\nround(Rg0,2) # for display only\n\n   0    1    2    3    4    5    6    7 \n1.00 1.00 0.70 0.49 0.34 0.23 0.16 0.11 \n\n\nAdd g(0) estimate to all segments and save final modeling data set\nLinear interpolation when Beaufort sea state changed during the segment.\n\nBFlow &lt;- floor(modeling_data$segdata$beaufort + 1)\nBFhigh &lt;- ceiling(modeling_data$segdata$beaufort + 1)\nw &lt;- BFhigh - (modeling_data$segdata$beaufort + 1)\nmodeling_data$segdata[,paste0(\"g0_w_\",sp.code)] &lt;- w\nmodeling_data$segdata[,paste0(\"g0\",\"_\",sp.code)] &lt;- w*Rg0[BFlow] + (1-w)*Rg0[BFhigh]\n\nsaveRDS(modeling_data, file.path(localwd, \"output\",\"seg_sight_out_g0_ESW.rds\"))\n\n\n\n\nC. Monte Carlo Estimation of ESW and g(0) Parameter Covariance Matrix\n\n# Design matrix for g(0) estimates \nLp &lt;- scam::predict.scam(g0_est, newdata = newdata_g0, type=\"lpmatrix\")\n# Subset for columns affecting g(0):\nbft_idx &lt;- grep(\"beaufort\", colnames(Lp))\nLp &lt;- Lp[,bft_idx]\n\n# Draw detection parameter sample:\ndetfun_g0 &lt;- detfun\nv_theta &lt;- solve(detfun_g0$ddf$hessian)\npars.esw &lt;- detfun_g0$ddf$par\n\n\nnpar &lt;- 10\nsamps &lt;- 500 # total samples = npar * samps\n\nplan(\"multisession\", workers=npar)\nset.seed(8675309)\n\ng0_par_sim &lt;- foreach(i = 1:npar,  .options.future = list(seed = TRUE), .errorhandling = \"pass\") %dofuture% {\n  theta_star &lt;- matrix(NA, samps, length(detfun$ddf$par))\n  v_alpha_star &lt;- matrix(NA, samps, length(bft_idx)^2)\n  alpha_star &lt;- matrix(NA, samps, length(bft_idx))\n  Rg0_star &lt;- matrix(NA, samps, 8)\n  # Constructing variance-covariance matrix \n  for(j in 1:samps){\n    g0_star_out &lt;- sim_esw_g0(pars.esw, v_theta, detfun, g0_dat)\n    g0_star &lt;- g0_star_out$g0_star\n    theta_star[j,] &lt;-  g0_star_out$theta_star\n    v_alpha_star[j,] &lt;- as.vector(g0_star$Vp.t[bft_idx, bft_idx])\n    alpha_star[j,] &lt;- g0_star$coefficients.t[bft_idx]\n    Rg0_star[j,] &lt;- g0_star_out$Rg0\n    \n  }\n  list(theta_star=theta_star, alpha_star=alpha_star, v_alpha_star=v_alpha_star, Rg0_star=Rg0_star)\n}\nplan(\"sequential\")\n\n\ntheta_star &lt;- lapply(g0_par_sim, \\(x) x$theta_star) %&gt;% do.call(rbind,.)\nalpha_star &lt;- lapply(g0_par_sim, \\(x) x$alpha_star) %&gt;% do.call(rbind,.)\nv_alpha_star &lt;- lapply(g0_par_sim, \\(x) x$v_alpha_star) %&gt;% do.call(rbind,.)\nRg0_star &lt;- lapply(g0_par_sim, \\(x) x$Rg0_star) %&gt;% do.call(rbind,.)\n\n## Compose full variance/covariance from law of total variance\nv_alpha &lt;- colMeans(v_alpha_star) %&gt;% matrix(.,sqrt(length(.))) +\n  var(alpha_star)\nc_alpha_theta &lt;- cov(alpha_star, theta_star)\nV &lt;- rbind(\n  cbind(v_alpha, c_alpha_theta),\n  cbind(t(c_alpha_theta), var(theta_star))\n)\n\nvarprop_list &lt;- list(\n  newdata_g0 = newdata_g0,\n  Lp_g0 = Lp,\n  V_par = V,\n  par_sim = cbind(alpha_star, theta_star),\n  Rg0_sim = Rg0_star,\n  detfun = detfun, \n  g0_dat = g0_dat,\n  g0_est = g0_est,\n  bft_idx = bft_idx\n)\n\nsaveRDS(varprop_list, file = file.path(datadir, \"output\",\"varprop_list.rds\"))",
    "crumbs": [
      "2. Estimate g(0) and ESW"
    ]
  }
]